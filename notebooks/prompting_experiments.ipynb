{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on prompting with Groq and LlamaIndex\n",
    "\n",
    "Groq - for deterministic and low latency execution. Need an API key (Open source)\n",
    "\n",
    "Zero shot prompting - Passing instructions, context and queries without any examples. Used when model can answer freely but within the context. Style of answer may vary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q -U llama-index llama-index-llms-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an API Key From Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core  import PromptTemplate\n",
    "from llama_index.llms.groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction is created once. This function can be called for any context and query\n",
    "def zero_shot_prompt(context, query, api_key):\n",
    "    llm = Groq(\n",
    "        api_key=api_key, model = \"llama-3.3-70b-versatile\", temperature=0\n",
    "    )\n",
    "    template_str = (\n",
    "        \"You are helpful assistant.\\n\"\n",
    "        \"Use only the context provided and answer the user's query.\"\n",
    "        \"If the context is not sufficient, reply 'Not enough information' \\n \"\n",
    "        \"Context:\\n{context_str}\\n\"\n",
    "        \"User Query:\\n{query_str}\\n\"\n",
    "        \"Rules for answering : \\n\"\n",
    "        \"1. Answer in 2-3 sentances precisely.\\n\"\n",
    "        \"2. Use bullet points for list.\\n\\n\"\n",
    "        \"Answer to the Query:\"\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(template_str).format(context_str=context, query_str=query)\n",
    "    response = llm.complete(prompt)\n",
    "    output_str = response.text\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers handle long-range dependencies using a self-attention mechanism. This mechanism allows each token to attend to all other tokens in the sequence. Key benefits of this approach include:\n",
      "* Capturing dependencies without recurrence, enabling more efficient processing.\n"
     ]
    }
   ],
   "source": [
    "# sample 1\n",
    "context_text = (\n",
    "    \"Transformers use a self-attention mechanism that allows each token \"\n",
    "    \"to attend to all other tokens in the sequence. This helps capture \"\n",
    "    \"long-range dependencies without recurrence.\"\n",
    ")\n",
    "\n",
    "query_text = \"How do Transformers handle long-range dependencies?\"\n",
    "\n",
    "answer_str = zero_shot_prompt(context=context_text, query=query_text, api_key=api_key)\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A knowledge graph helps trace the cause of a defect in a factory by linking defects to specific components and machines. This is achieved through relationships such as:\n",
      "* component causes defect\n",
      "* machine uses component. By doing so, teams can understand how problems propagate in the system and identify the root cause of a defect.\n"
     ]
    }
   ],
   "source": [
    "# sample 2\n",
    "context_text = (\n",
    "    \"A knowledge graph stores information as entities and relationships.\" \n",
    "    \"In a factory setting, entities include machines, components, and \"\n",
    "    \"defects. Relationships describe how entities are connected, such as \" \n",
    "    \"machine uses component or component causes defect. By linking defects \"\n",
    "    \"to specific components and machines, the knowledge graph enables tracing \"\n",
    "    \"connections across the production process. This structured representation \"\n",
    "    \"helps teams understand how problems propagate in the system.\"\n",
    ")\n",
    "query_text = \"How a knowledge graph helps trace the cause of a defect in a factory.\"\n",
    "answer_str = zero_shot_prompt(context=context_text, query=query_text)\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction is created once. This function can be called for any context, query and example\n",
    "\n",
    "def few_shot_prompt(context, query, example):\n",
    "    llm = Groq(\n",
    "        model = \"llama-3.3-70b-versatile\", temperature = 0\n",
    "    )\n",
    "    template_str = (\n",
    "        \"You are helpful assistant.\\n\"\n",
    "        \"Use only the context provided and answer the user's query.\"\n",
    "        \"If the context is not sufficient, reply 'Not enough information' \\n \"\n",
    "        \"Follow the examples provided to generate the resule\"\n",
    "        \"Examples:\\n{example_str}\\n\"\n",
    "        \"Context:\\n{context_str}\\n\"\n",
    "        \"User Query:\\n{query_str}\\n\"\n",
    "        \"Rules for answering : \\n\"\n",
    "        \"1. Answer in 2-3 sentances precisely.\\n\"\n",
    "        \"2. Use bullet points for list.\\n\\n\"\n",
    "        \"3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\\n\\n\"\n",
    "        \"Answer to the Query:\"\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(template_str).format(example_str=example, context_str= context, query_str=query)\n",
    "    response = llm.complete(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot: Add examples so the model mimics your style\n",
    "shots = [\n",
    "    {\n",
    "        \"context\": \"Positional encodings inject order information into sequences.\",\n",
    "        \"question\": \"Why are positional encodings needed?\",\n",
    "        \"answer\": (\n",
    "            \"They give the model a sense of word order.\\n\"\n",
    "            \"- Without them, the model treats tokens as a bag of words.\\n\"\n",
    "            \"- Encodings ensure the sequence structure is preserved.\\n\"\n",
    "            \"Sources: lecture_notes.txt\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Multi-head attention projects queries, keys, and values into multiple subspaces.\",\n",
    "        \"question\": \"What is the benefit of multi-head attention?\",\n",
    "        \"answer\": (\n",
    "            \"It lets the model learn from different representation subspaces.\\n\"\n",
    "            \"- Captures diverse relationships.\\n\"\n",
    "            \"- Improves contextual understanding.\\n\"\n",
    "            \"Sources: attention_paper.pdf\"\n",
    "        )\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A knowledge graph helps trace the cause of a defect in a factory by linking defects to specific components and machines. This is achieved through relationships such as:\n",
      "* machine uses component\n",
      "* component causes defect\n",
      "By doing so, it enables teams to understand how problems propagate in the system. \n",
      "Sources: knowledge_graph_description.txt\n"
     ]
    }
   ],
   "source": [
    "context_text = (\n",
    "    \"A knowledge graph stores information as entities and relationships.\" \n",
    "    \"In a factory setting, entities include machines, components, and \"\n",
    "    \"defects. Relationships describe how entities are connected, such as \" \n",
    "    \"machine uses component or component causes defect. By linking defects \"\n",
    "    \"to specific components and machines, the knowledge graph enables tracing \"\n",
    "    \"connections across the production process. This structured representation \"\n",
    "    \"helps teams understand how problems propagate in the system.\"\n",
    ")\n",
    "query_text = \"How a knowledge graph helps trace the cause of a defect in a factory.\"\n",
    "answer_str = few_shot_prompt(context=context_text, query=query_text, example=shots)\n",
    "print(answer_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
